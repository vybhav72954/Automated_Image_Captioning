# -*- coding: utf-8 -*-
"""caption_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oSUxHpz6UIEhtGjOw_SCAYWxYodwCYpf
"""

# from google.colab import drive
# drive.mount("/content/gdrive")

import string
import numpy as np
from PIL import Image
import os
from pickle import dump, load
import numpy as np
import pydot
from keras.applications.xception import Xception, preprocess_input
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout

# small library for seeing the progress of loops.
from tqdm.notebook import tqdm
import tensorflow as tf
with tf.device('/gpu:0'):
    config = tf.compat.v1.ConfigProto(gpu_options=
                                      tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=1)
                                      # device_count = {'GPU': 1}
                                      )
    config.gpu_options.allow_growth = True
    session = tf.compat.v1.Session(config=config)
    tf.compat.v1.keras.backend.set_session(session)
    tqdm().pandas()
    """
    import tensorflow as tf
    device_name = tf.test.gpu_device_name()
    if device_name != '/device:GPU:0':
      raise SystemError('GPU device not found')
    print('Found GPU at: {}'.format(device_name))
    """

    # Loading a text file into memory
    def load_doc(filename):
        # Opening the file as read only
        file = open(filename, 'r')
        text = file.read()
        file.close()
        return text


    # get all imgs with their captions
    def all_img_captions(filename):
        file = load_doc(filename)
        captions = file.split('\n')
        descriptions = {}
        for caption in captions[:-1]:
            img, caption = caption.split('\t')
            if img[:-2] not in descriptions:
                descriptions[img[:-2]] = [caption]
            else:
                descriptions[img[:-2]].append(caption)
        return descriptions


    ##Data cleaning- lower casing, removing puntuations and words containing numbers
    def cleaning_text(captions):
        table = str.maketrans('', '', string.punctuation)
        for img, caps in captions.items():
            for i, img_caption in enumerate(caps):
                img_caption.replace("-", " ")
                desc = img_caption.split()

                # converts to lower case
                desc = [word.lower() for word in desc]
                # remove punctuation from each token
                desc = [word.translate(table) for word in desc]
                # remove hanging 's and a
                desc = [word for word in desc if (len(word) > 1)]
                # remove tokens with numbers in them
                desc = [word for word in desc if (word.isalpha())]
                # convert back to string

                img_caption = ' '.join(desc)
                captions[img][i] = img_caption
        return captions


    def text_vocabulary(descriptions):
        # build vocabulary of all unique words
        vocab = set()

        for key in descriptions.keys():
            [vocab.update(d.split()) for d in descriptions[key]]

        return vocab


    # All descriptions in one file
    def save_descriptions(descriptions, filename):
        lines = list()
        for key, desc_list in descriptions.items():
            for desc in desc_list:
                lines.append(key + '\t' + desc)
        data = "\n".join(lines)
        file = open(filename, "w")
        file.write(data)
        file.close()


    # Set these path according to project folder in you system
    # dataset_text = "/content/gdrive/MyDrive/Flickr8k_text"
    # dataset_images = "/content/gdrive/MyDrive/Flickr8k_Dataset/img"

    dataset_text = "Flickr8k_text"
    dataset_images = "Flickr8k_Dataset/img"

    # we prepare our text data
    filename = dataset_text + "/" + "Flickr8k.token.txt"
    # loading the file that contains all data
    # mapping them into descriptions dictionary img to 5 captions
    descriptions = all_img_captions(filename)
    print("Length of descriptions =", len(descriptions))

    # cleaning the descriptions
    clean_descriptions = cleaning_text(descriptions)

    # building vocabulary
    vocabulary = text_vocabulary(clean_descriptions)
    print("Length of vocabulary = ", len(vocabulary))

    # saving each description to file
    save_descriptions(clean_descriptions, "Model/descriptions.txt")

    """
    def extract_features(directory):
        with tf.device('/device:GPU:0'):
            model = Xception(include_top=False, pooling='avg')
            features = {}
            for img in tqdm(os.listdir(directory)):
                filename = directory + "/" + img
                image = Image.open(filename)
                image = image.resize((299, 299))
                image = np.expand_dims(image, axis=0)
                # image = preprocess_input(image)
                image = image / 127.5
                image = image - 1.0
    
                feature = model.predict(image)
                features[img] = feature
            return features
    
    
    # 2048 feature vector
    features = extract_features(dataset_images)
    dump(features, open("features_GPU.p", "wb"))
    """

    features = load(open("features_GPU.p","rb"))

    # load the data
    def load_photos(filename):
        file = load_doc(filename)
        photos = file.split("\n")[:-1]
        return photos


    def load_clean_descriptions(filename, photos):
        # loading clean_descriptions
        file = load_doc(filename)
        descriptions = {}
        for line in file.split("\n"):

            words = line.split()
            if len(words) < 1:
                continue

            image, image_caption = words[0], words[1:]

            if image in photos:
                if image not in descriptions:
                    descriptions[image] = []
                desc = '<start> ' + " ".join(image_caption) + ' <end>'
                descriptions[image].append(desc)

        return descriptions


    def load_features(photos):
        # loading all features
        all_features = load(open("features_GPU.p", "rb"))
        # selecting only needed features
        features = {k: all_features[k] for k in photos}
        return features


    filename = dataset_text + "/" + "Flickr_8k.trainImages.txt"

    # train = loading_data(filename)
    train_imgs = load_photos(filename)
    train_descriptions = load_clean_descriptions("Model/descriptions.txt", train_imgs)
    train_features = load_features(train_imgs)


    # converting dictionary to clean list of descriptions
    def dict_to_list(descriptions):
        all_desc = []
        for key in descriptions.keys():
            [all_desc.append(d) for d in descriptions[key]]
        return all_desc


    # creating tokenizer class
    # this will vectorise text corpus
    # each integer will represent token in dictionary

    from keras.preprocessing.text import Tokenizer


    def create_tokenizer(descriptions):
        desc_list = dict_to_list(descriptions)
        tokenizer = Tokenizer()
        tokenizer.fit_on_texts(desc_list)
        return tokenizer


    # give each word an index, and store that into tokenizer.p pickle file
    tokenizer = create_tokenizer(train_descriptions)
    dump(tokenizer, open('Model/tokenizer.p', 'wb'))
    vocab_size = len(tokenizer.word_index) + 1
    vocab_size


    # calculate maximum length of descriptions
    def max_length(descriptions):
        desc_list = dict_to_list(descriptions)
        return max(len(d.split()) for d in desc_list)


    max_length = max_length(descriptions)
    max_length

    features['1000268201_693b08cb0e.jpg'][0]


    # data generator, used by model.fit_generator()

    # data generator, used by model.fit_generator()
    def data_generator(descriptions, features, tokenizer, max_length):
        while 1:
            for key, description_list in descriptions.items():
                # retrieve photo features
                feature = features[key][0]
                input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list,
                                                                            feature)
                yield [input_image, input_sequence], output_word


    def create_sequences(tokenizer, max_length, desc_list, feature):
        X1, X2, y = list(), list(), list()
        # walk through each description for the image
        for desc in desc_list:
            # encode the sequence
            seq = tokenizer.texts_to_sequences([desc])[0]
            # split one sequence into multiple X,y pairs
            for i in range(1, len(seq)):
                # split into input and output pair
                in_seq, out_seq = seq[:i], seq[i]
                # pad input sequence
                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                # encode output sequence
                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
                # store
                X1.append(feature)
                X2.append(in_seq)
                y.append(out_seq)
        return np.array(X1), np.array(X2), np.array(y)


    # You can check the shape of the input and output for your model
    [a, b], c = next(data_generator(train_descriptions, features, tokenizer, max_length))
    a.shape, b.shape, c.shape

    from keras.utils import plot_model

    with tf.device('/device:GPU:0'):
        # define the captioning model
        def define_model(vocab_size, max_length):
            # features from the CNN model squeezed from 2048 to 256 nodes
            inputs1 = Input(shape=(2048,))
            fe1 = Dropout(0.5)(inputs1)
            fe2 = Dense(256, activation='relu')(fe1)

            # LSTM sequence model
            inputs2 = Input(shape=(max_length,))
            se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
            se2 = Dropout(0.5)(se1)
            se3 = LSTM(256)(se2)

            # Merging both models
            decoder1 = add([fe2, se3])
            decoder2 = Dense(256, activation='relu')(decoder1)
            outputs = Dense(vocab_size, activation='softmax')(decoder2)

            # tie it together [image, seq] [word]
            model = Model(inputs=[inputs1, inputs2], outputs=outputs)
            model.compile(loss='categorical_crossentropy', optimizer='adam')

            # summarize model
            print(model.summary())
            plot_model(model, to_file='Model/model.png', show_shapes=True)

            return model

    print('Dataset: ', len(train_imgs))
    print('Descriptions: train=', len(train_descriptions))
    print('Photos: train=', len(train_features))
    print('Vocabulary Size:', vocab_size)
    print('Description Length: ', max_length)

    model = define_model(vocab_size, max_length)
    epochs = 10
    steps = len(train_descriptions)
    os.mkdir("models6")
    for i in range(epochs):
        generator = data_generator(train_descriptions, train_features, tokenizer, max_length)
        model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)
        model.save("models6/model_" + str(i) + ".h5")
